[![Code-Generator](https://badgen.net/badge/Template%20by/Code-Generator/ee4c2c?labelColor=eaa700)](https://github.com/pytorch-ignite/code-generator)

# Single Model, Single Optimizer Template

> Generated by [Code-Generator](https://github.com/pytorch-ignite/code-generator)

This is a template generated for single model, single optimizer based training.

<details>
<summary>
Table of Contents
</summary>

- [Getting Started](#getting-started)
- [Training](#training)

</details>

## Getting Started

- Install the dependencies with `pip`:

  ```sh
  pip install -r requirements.txt --progress-bar off
  ```

- Edit `datasets.py` for your custom datasets and dataloaders
- Edit `engines.py` for your custom models' forward pass, backward pass, and evaluation
- Edit `models.py` for your custom models
- Extend `utils.py` for additional command line arguments
- Extend `handlers.py` for your custom handlers _(**Optional**)_

## Training

### Single Node, Single GPU

```sh
python main.py --verbose
```

### Single Node, Multiple GPUs

- Using function spawn inside the code

  ```sh
  python main.py run --backend="nccl" --nproc_per_node=2
  ```

- Using `torch.distributed.launch`

  ```sh
  python -u -m torch.distributed.launch --nproc_per_node=2 --use_env main.py run --backend="nccl"
  ```

### Colab 8 TPUs

### Multiple Nodes, Multiple GPUs

Let's start training on two nodes with 2 gpus each. We assuming that master node can be connected as master, e.g. ping master.

- Execute on master node

  ```sh
  python -u -m torch.distributed.launch \
    --nnodes=2 \
    --nproc_per_node=2 \
    --node_rank=0 \
    --master_addr=master --master_port=2222 --use_env \
    main.py run --backend="nccl"
  ```

- Execute on worker node

  ```sh
  python -u -m torch.distributed.launch \
    --nnodes=2 \
    --nproc_per_node=2 \
    --node_rank=1 \
    --master_addr=master --master_port=2222 \
    main.py run --backend="nccl"
  ```
