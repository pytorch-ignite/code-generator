{% block imports %}
from argparse import ArgumentParser

from ignite.engine.engine import Engine

import torch
{% endblock %}

{% block get_default_parser %}
DEFAULTS = {
    "amp_mode": {
        "default": "{{ amp_mode }}",
        "type": str,
        "help": "to use torch.cuda.amp or apex",
    },
    "train_batch_size": {
        "default": {{ train_batch_size }},
        "type": int,
        "help": "will be equally divided by number of GPUs if in distributed ({{ train_batch_size }})",
    },
    "data_path": {
        "default": "{{ data_path }}",
        "type": str,
        "help": "datasets path ({{ data_path }})",
    },
    "device": {
        "default": "{{ device }}",
        "type": torch.device,
        "help": "device to use for training / evaluation / testing ({{ device }})",
    },
    "filepath": {
        "default": "{{ filepath }}",
        "type": str,
        "help": "logging file path ({{ filepath }})",
    },
    "num_workers": {
        "default": {{ num_workers }},
        "type": int,
        "help": "num_workers for DataLoader ({{ num_workers }})",
    },
    "max_epochs": {
        "default": {{ max_epochs }},
        "type": int,
        "help": "max_epochs of ignite.Engine.run() ({{ max_epochs }})",
    },
    "lr": {
        "default": {{ lr }},
        "type": float,
        "help": "learning rate used by torch.optim.* ({{ lr }})",
    },
    "log_train": {
        "default": {{ log_train }},
        "type": int,
        "help": "logging interval of training iteration ({{ log_train }})",
    },
    "log_eval": {
        "default": {{ log_eval }},
        "type": int,
        "help": "logging interval of evaluation epoch ({{ log_eval }})",
    },
    "seed": {
        "default": {{ seed }},
        "type": int,
        "help": "used in ignite.utils.manual_seed() ({{ seed }})",
    },
    "eval_batch_size": {
        "default": {{ eval_batch_size }},
        "type": int,
        "help": "will be equally divided by number of GPUs if in distributed ({{ eval_batch_size }})",
    },
    "verbose": {
        "action": "store_true",
        "help": "use logging.INFO in ignite.utils.setup_logger",
    },
    "backend": {
        "default": "{{ backend }}",
        "type": str,
        "help": "backend to use: nccl, gloo, xla-tpu, horovod. ({{ backend }})"
    },
    "nproc_per_node": {
        "default": {{ None if nproc_per_node == 0 else nproc_per_node }},
        "type": int,
        "help": """number of processes to launch on each node, for GPU training
                this is recommended to be set to the number of GPUs in your system
                so that each process can be bound to a single GPU ({{ nproc_per_node }})""",
    },
    "nnodes": {
        "default": {{ None if nnodes == 0 else nnodes }},
        "type": int,
        "help": "number of nodes to use for distributed training ({{ nnodes }})",
    },
    "node_rank": {
        "default": {{ None if node_rank == 0 else node_rank }},
        "type": int,
        "help": "rank of the node for multi-node distributed training ({{ node_rank }})",
    },
}


def get_default_parser():
    """Get the default configs for training."""
    parser = ArgumentParser(add_help=False)

    for key, value in DEFAULTS.items():
        parser.add_argument(f"--{key}", **value)

    return parser
{% endblock %}


{% block log_metrics %}
def log_metrics(engine: Engine, tag: str, device: torch.device) -> None:
    """Log ``engine.state.metrics`` with given ``engine``
    and memory info with given ``device``.

    Args:
        engine (Engine): instance of ``Engine`` which metrics to log.
        tag (str): a string to add at the start of output.
        device (torch.device): current torch.device to log memory info.
    """
    max_epochs = len(str(engine.state.max_epochs))
    max_iters = len(str(engine.state.max_iters))
    metrics_format = "{tag} [{epoch:>{max_epochs}}/{iteration:0{max_iters}d}]: {metrics}".format(
        tag=tag,
        epoch=engine.state.epoch,
        max_epochs=max_epochs,
        iteration=engine.state.iteration,
        max_iters=max_iters,
        metrics=engine.state.metrics,
    )
    if "cuda" in device.type:
        metrics_format += " Memory - {:.2f} MB".format(
            torch.cuda.max_memory_allocated(device) / (1024.0 * 1024.0)
        )
    engine.logger.info(metrics_format)
{% endblock %}
