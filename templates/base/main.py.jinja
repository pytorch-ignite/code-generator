{% block imports %}
from argparse import ArgumentParser
from datetime import datetime
from pathlib import Path
from typing import Any

import logging

import ignite.distributed as idist
import torch
from ignite.engine import Engine
from ignite.engine.events import Events
from ignite.utils import setup_logger, manual_seed
from ignite.metrics import Accuracy
from torch import optim, nn

from datasets import get_datasets, get_data_loaders
from models import get_model
from fn import train_fn, evaluate_fn
from utils import log_metrics, get_default_parser
{% endblock %}


{% block run %}
def run(local_rank: int, config: Any, *args: Any, **kwags: Any):

    # -----------------------------
    # datasets and dataloaders
    # -----------------------------
    {% block datasets_and_dataloaders %}
    train_dataset, eval_dataset = get_datasets()
    train_dataloader, eval_dataloader = get_data_loaders(
        train_dataset=train_dataset,
        eval_dataset=eval_dataset,
        train_batch_size=config.train_batch_size,
        eval_batch_size=config.eval_batch_size,
        num_workers=config.num_workers,
    )
    {% endblock %}

    # ------------------------------------------
    # model, optimizer, loss function, device
    # ------------------------------------------
    {% block model_optimizer_loss %}
    model = idist.auto_model(get_model())
    model.weight.data.zero_()
    model.bias.data.zero_()
    optimizer = idist.auto_optim(optim.Adam(model.parameters(), lr=config.lr))
    loss_fn = nn.MSELoss()
    {% endblock %}

    # ----------------------
    # train / eval engine
    # ----------------------
    {% block engines %}
    train_engine = Engine(train_fn(model, optimizer, loss_fn, config.device, config, True))
    eval_engine = Engine(evaluate_fn(model, config.device, config, True))
    {% endblock %}

    # ------------------
    # attach metrics
    # ------------------
    {% block metrics %}
    def thresholded_output_transform(output):
        y_pred, y = output
        y_pred = torch.round(y_pred)
        y = torch.round(y)
        return y_pred, y
    accuracy = Accuracy(output_transform=thresholded_output_transform, device=config.device)
    accuracy.attach(eval_engine, "eval_accuracy")
    {% endblock %}

    # ---------------
    # setup logging
    # ---------------
    {% block loggers %}
    name = f"bs{config.train_batch_size}-lr{config.lr}-{optimizer.__class__.__name__}"
    now = datetime.now().strftime("%Y%m%d-%X")
    train_engine.logger = setup_logger("trainer", level=config.verbose, filepath=config.filepath / f"{name}-{now}.log")
    eval_engine.logger = setup_logger("evaluator", level=config.verbose, filepath=config.filepath / f"{name}-{now}.log")
    {% endblock %}

    # ----------------------
    # engines log and run
    # ----------------------
    {% block engines_run_and_log %}
    {% block log_training_results %}
    @train_engine.on(Events.ITERATION_COMPLETED(every=config.log_train))
    def log_training_results(engine):
        train_engine.state.metrics = train_engine.state.output
        log_metrics(train_engine, "Train", config.device)
    {% endblock %}

    {% block run_eval_engine_and_log %}
    @train_engine.on(Events.EPOCH_COMPLETED(every=config.log_eval))
    def run_eval_engine_and_log(engine):
        eval_engine.run(eval_dataloader)
        log_metrics(eval_engine, "Eval", config.device)
    {% endblock %}

    train_engine.run(train_dataloader, max_epochs=config.max_epochs)
    {% endblock %}
{% endblock %}


{% block main_fn %}
def main():
    parser = ArgumentParser(parents=[get_default_parser()])
    config = parser.parse_args()
    manual_seed(config.seed)
    config.verbose = logging.INFO if config.verbose else logging.WARNING
    if config.filepath:
        path = Path(config.filepath)
        path.mkdir(parents=True, exist_ok=True)
        config.filepath = path
    with idist.Parallel(
        backend=idist.backend(),
        nproc_per_node=config.nproc_per_node,
        nnodes=config.nnodes,
        node_rank=config.node_rank,
        master_addr=config.master_addr,
        master_port=config.master_port
    ) as parallel:
        parallel.run(run, config=config)
{% endblock %}


{% block entrypoint %}
if __name__ == "__main__":
    main()
{% endblock %}
